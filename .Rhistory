S0 <- L0 <- matrix(c(625,312.5,312.5,625), nrow=2, ncol=2)
set.seed(2021)
for(i in 1:sample.size){
# update theta
Ln = inv(inv(L0) + n*inv(Sigma))
mun = Ln %*% (inv(L0)%*%mu0 + n*inv(Sigma)%*%ybar)
theta = mvrnorm(1, mun, Ln)
# update sigma
Sn = S0 + (t(test)-theta)%*%t(t(test)-theta)
Sigma = inv(rWishart(1, nu0+n, inv(Sn))[,,1])
# Save results
THETA <- rbind(THETA, theta)
SIGMA <- rbind(SIGMA, c(Sigma))
# sample new
sample.new = rbind(sample.new, mvrnorm(n=1, mu=theta, Sigma=Sigma))
}
rownames(THETA) <- 1:sample.size
rownames(SIGMA) <- 1:sample.size
# graph
Theta
# graph
THETA
# graph
names(THETA)
# graph
namesdata.frame(THETA)()
# graph
names(data.frame(THETA))
# graph
data.frame(THETA) %>%
ggplot(aes(x=pretest, y=posttest)) + geom_point()
# graph
data.frame(THETA) %>%
ggplot(aes(x=pretest, y=posttest)) + geom_point(size=1, color='orange')
# graph
data.frame(THETA) %>%
ggplot(aes(x=pretest, y=posttest)) + geom_point(size=1, color='green')
# graph
data.frame(THETA) %>%
ggplot(aes(x=pretest, y=posttest)) + geom_point(size=1, color='darkgreen')
# graph
data.frame(THETA) %>%
ggplot(aes(x=pretest, y=posttest)) + geom_point(size=1, color='oragne')
# graph
data.frame(THETA) %>%
ggplot(aes(x=pretest, y=posttest)) + geom_point(size=1, color='orange')
# graph
data.frame(THETA) %>%
ggplot(aes(x=pretest, y=posttest)) + geom_point(size=1, color='orange') +
geom_abline(slope=1, intercept=0)
# graph
data.frame(THETA) %>%
ggplot(aes(x=pretest, y=posttest)) + geom_point(size=1, color='orange') +
geom_abline(slope=1, intercept=0) + coord_fixed(ratio=1)
# graph
data.frame(THETA) %>%
ggplot(aes(x=pretest, y=posttest)) + geom_point(size=1, color='orange') +
geom_abline(slope=1, intercept=0) + coord_fixed(ratio=2)
# graph
data.frame(THETA) %>%
ggplot(aes(x=pretest, y=posttest)) + geom_point(size=1, color='orange') +
geom_abline(slope=1, intercept=0) + coord_fixed(ratio=3)
# graph
data.frame(THETA) %>%
ggplot(aes(x=pretest, y=posttest)) + geom_point(size=1, color='orange') +
geom_abline(slope=1, intercept=0) + coord_fixed(ratio=1)
# graph
data.frame(THETA) %>%
ggplot(aes(x=pretest, y=posttest)) +
geom_point(size=1, color='orange') +
geom_abline(slope=1, intercept=0) +
coord_fixed(ratio=1) +
ggtitle('Posterior draws of Mu')
sample.new
data.frame(sample.new)
names(data.frame(sample.new))
data.frame(sample.new) %>%
ggplot(aes(x=pretest, y=posttest)) +
geom_point(size=1, color='orange') +
geom_abline(slope=1, intercept=0) +
coord_fixed(ratio=1) +
ggtitle('Posterior Predictive')
data.frame(sample.new) %>%
ggplot(aes(x=pretest, y=posttest)) +
geom_point(size=1, color='orange') +
geom_abline(slope=1, intercept=0) +
# coord_fixed(ratio=1) +
ggtitle('Posterior Predictive')
grid.arrange(p1, p2, nrow=1)
# graph
p1 <- data.frame(THETA) %>%
ggplot(aes(x=pretest, y=posttest)) +
geom_point(size=1, color='orange') +
geom_abline(slope=1, intercept=0) +
ggtitle('Posterior draws of Mu')
p2 <- data.frame(sample.new) %>%
ggplot(aes(x=pretest, y=posttest)) +
geom_point(size=1, color='orange') +
geom_abline(slope=1, intercept=0) +
ggtitle('Posterior Predictive')
grid.arrange(p1, p2, nrow=1)
# graph(ggplot 활용)
p1 <- data.frame(THETA) %>%
ggplot(aes(x=pretest, y=posttest)) +
geom_point(size=1, color='orange') +
geom_abline(slope=1, intercept=0) +
xlab(expression(theta)) +
ggtitle('Posterior draws of Mu')
# graph(ggplot 활용)
data.frame(THETA) %>%
ggplot(aes(x=pretest, y=posttest)) +
geom_point(size=1, color='orange') +
geom_abline(slope=1, intercept=0) +
xlab(expression(theta)) +
ggtitle('Posterior draws of Mu')
# graph(ggplot 활용)
data.frame(THETA) %>%
ggplot(aes(x=pretest, y=posttest)) +
geom_point(size=1, color='orange') +
geom_abline(slope=1, intercept=0) +
xlab(expression(theta_1)) +
ggtitle('Posterior draws of Mu')
# graph(ggplot 활용)
data.frame(THETA) %>%
ggplot(aes(x=pretest, y=posttest)) +
geom_point(size=1, color='orange') +
geom_abline(slope=1, intercept=0) +
xlab(expression(theta)_1) +
ggtitle('Posterior draws of Mu')
# graph(ggplot 활용)
data.frame(THETA) %>%
ggplot(aes(x=pretest, y=posttest)) +
geom_point(size=1, color='orange') +
geom_abline(slope=1, intercept=0) +
xlab(expression(theta)_1) +
ggtitle('Posterior draws of Mu')
# graph(ggplot 활용)
data.frame(THETA) %>%
ggplot(aes(x=pretest, y=posttest)) +
geom_point(size=1, color='orange') +
geom_abline(slope=1, intercept=0) +
xlab(expression(theta[1])) +
ggtitle('Posterior draws of Mu')
# graph(ggplot 활용)
data.frame(THETA) %>%
ggplot(aes(x=pretest, y=posttest)) +
geom_point(size=1, color='orange') +
geom_abline(slope=1, intercept=0) +
xlab(expression(theta[1])) + ylab(expression(theta[2])) +
ggtitle('Posterior draws of Mu')
p2 <- data.frame(sample.new) %>%
ggplot(aes(x=pretest, y=posttest)) +
geom_point(size=1, color='orange') +
geom_abline(slope=1, intercept=0) +
xlab(expression(y[1])) + ylab(expression(y[2])) +
ggtitle('Posterior Predictive')
grid.arrange(p1, p2, nrow=1)
# graph(ggplot 활용)
p1 <- data.frame(THETA) %>%
ggplot(aes(x=pretest, y=posttest)) +
geom_point(size=1, color='orange') +
geom_abline(slope=1, intercept=0) +
xlab(expression(theta[1])) + ylab(expression(theta[2])) +
ggtitle('Posterior draws of Mu')
p2 <- data.frame(sample.new) %>%
ggplot(aes(x=pretest, y=posttest)) +
geom_point(size=1, color='orange') +
geom_abline(slope=1, intercept=0) +
xlab(expression(y[1])) + ylab(expression(y[2])) +
ggtitle('Posterior Predictive')
grid.arrange(p1, p2, nrow=1)
grid.arrange(p1, p2, nrow=1) +
ggtitle('Reading Comprehension')
grid.arrange(p1, p2, nrow=1)
install.packages('rmd2jupyter')
getwd()
library(rmd2jupyter)
rmd2jupyter('new.Rmd')
#### Diabetes example
load("diabetes.RData")
yf<-diabetes$y
yf<-(yf-mean(yf))/sd(yf)
Xf<-diabetes$X
Xf<-t( (t(Xf)-apply(Xf,2,mean))/apply(Xf,2,sd))
## set up training and test data
n<-length(yf)
set.seed(1)
i.te<-sample(1:n,100)
i.tr<-(1:n)[-i.te]
y<-yf[i.tr] ; y.te<-yf[i.te]
X<-Xf[i.tr,]; X.te<-Xf[i.te,]
#### Bayesian model selection
p<-dim(X)[2]
S<-10000
source("regression_gprior.R")
## Don't run it again if you've already run it
runmcmc<-!any(system("ls",intern=TRUE)=="diabetesBMA.RData")
if(!runmcmc){ load("diabetesBMA.RData") }
if(runmcmc){
BETA<-Z<-matrix(NA,S,p)
z<-rep(1,dim(X)[2] )
lpy.c<-lpy.X(y,X[,z==1,drop=FALSE])
for(s in 1:S)
{
for(j in sample(1:p))
{
zp<-z ; zp[j]<-1-zp[j]
lpy.p<-lpy.X(y,X[,zp==1,drop=FALSE])
r<- (lpy.p - lpy.c)*(-1)^(zp[j]==0)
z[j]<-rbinom(1,1,1/(1+exp(-r)))
if(z[j]==zp[j]) {lpy.c<-lpy.p}
}
beta<-z
if(sum(z)>0){beta[z==1]<-lm.gprior(y,X[,z==1,drop=FALSE],S=1)$beta }
Z[s,]<-z
BETA[s,]<-beta
if(s%%10==0)
{
bpm<-apply(BETA[1:s,],2,mean) ; plot(bpm)
cat(s,mean(z), mean( (y.te-X.te%*%bpm)^2),"\n")
Zcp<- apply(Z[1:s,,drop=FALSE],2,cumsum)/(1:s)
plot(c(1,s),range(Zcp),type="n") ; apply(Zcp,2,lines)
}
}
save(BETA,Z,file="diabetesBMA.RData")
}
#### Figure 9.7
par(mar=c(2.75,2.75,.5,.5),mgp=c(1.7,.7,0))
beta.bma<-apply(BETA,2,mean,na.rm=TRUE)
y.te.bma<-X.te%*%beta.bma
mean( (y.te-y.te.bma)^2)
layout( matrix(c(1,1,2),nrow=1,ncol=3) )
plot(apply(Z,2,mean,na.rm=TRUE),xlab="regressor index",ylab=expression(
paste( "Pr(",italic(z[j] == 1),"|",italic(y),",X)",sep="")),type="h",lwd=2)
plot(y.te,y.te.bma,xlab=expression(italic(y)[test]),
ylab=expression(hat(italic(y))[test])) ; abline(0,1)
rm(list=ls())
library(knitr)
knit('bayes_week5.Rmd')
?knit
knit('bayes_week5.Rmd', output = 'pdf_document')
?knit
diabetes
diabetes
View(diabetes)
diabetes[["X"]]
diabetes$X
dim(diabetes$X)
View(diabetes$X)
write.csv(diabetes$X, file = 'diabetes.x')
write.csv(diabetes$X, file = 'diabetes_x.csv')
write.csv(diabetes$y, file = 'diabetes_y.csv')
#### Diabetes example
load("data/diabetes.RData")
yf<-diabetes$y
yf<-(yf-mean(yf))/sd(yf)
Xf<-diabetes$X
Xf<-t( (t(Xf)-apply(Xf,2,mean))/apply(Xf,2,sd))
# Train-Test split
## set up training and test data
n<-length(yf)
set.seed(1)
i.te<-sample(1:n,100)
i.tr<-(1:n)[-i.te]
y<-yf[i.tr] ; y.te<-yf[i.te]
X<-Xf[i.tr,]; X.te<-Xf[i.te,]
#### Diabetes example
load("data/diabetes.RData")
yf<-diabetes$y
yf<-(yf-mean(yf))/sd(yf)
Xf<-diabetes$X
Xf<-t( (t(Xf)-apply(Xf,2,mean))/apply(Xf,2,sd))
# Train-Test split
## set up training and test data
n<-length(yf)
set.seed(1)
i.te<-sample(1:n,100)
i.tr<-(1:n)[-i.te]
y<-yf[i.tr] ; y.te<-yf[i.te]
X<-Xf[i.tr,]; X.te<-Xf[i.te,]
# Bayesian Model Selection
#### Bayesian model selection
p<-dim(X)[2]
S<-10000
# source("regression_gprior.R")
lm.gprior<-function(y,X,g=dim(X)[1],nu0=1,s20=try(summary(lm(y~-1+X))$sigma^2,silent=TRUE),S=1000)
{
n<-dim(X)[1] ; p<-dim(X)[2]
Hg<- (g/(g+1)) * X%*%solve(t(X)%*%X)%*%t(X)
SSRg<- t(y)%*%( diag(1,nrow=n)  - Hg ) %*%y
s2<-1/rgamma(S, (nu0+n)/2, (nu0*s20+SSRg)/2 )
Vb<- g*solve(t(X)%*%X)/(g+1)
Eb<- Vb%*%t(X)%*%y
E<-matrix(rnorm(S*p,0,sqrt(s2)),S,p)
beta<-t(  t(E%*%chol(Vb)) +c(Eb))
list(beta=beta,s2=s2)
}
lmratio.gprior<-function(z0,z1,y,X,g=dim(X)[1],nu0=1,
s200=mean( lm(y~-1+X[,z0==1])$res^2),
s201=mean( lm(y~-1+X[,z1==1])$res^2) )
{
n<-dim(X)[1]
X0<-X[,z0==1]
X1<-X[,z1==1]
H0<- (g/(g+1)) * X0%*%solve(t(X0)%*%X0)%*%t(X0)
SS0<- t(y)%*%( diag(1,nrow=n)  - H0 ) %*%y
p0<-sum(z0==1)
H1<- (g/(g+1)) * X1%*%solve(t(X1)%*%X1)%*%t(X1)
SS1<- t(y)%*%( diag(1,nrow=n)  - H1 ) %*%y
p1<-sum(z1==1)
-.5*(p1-p0)*log( 2*pi*(1+g))  +
.5*nu0*log(s201/s200) + .5*(nu0+n)*log( (nu0*s200+SS0)/(nu0+s201+SS1) )
}
mselect.gprior<-function(y,X,S=500*dim(X)[2],verb=FALSE)
{
Z<-NULL
z<-rep(1,dim(X)[2] )
for(s in 1:S)
{
for(j in sample(1:p))
{
z1<-z0<-z  ; z1[j]<-1 ; z0[j]<-0
r<-lmratio.gprior(z0,z1,y,X)
z[j]<-rbinom(1,1,1/(1+exp(-r)))
}
Z<-rbind(Z,z)
if(verb==TRUE) {cat(s,mean(z),"\n") }
}
Z
}
lpy.X<-function(y,X,
g=length(y),nu0=1,s20=try(summary(lm(y~-1+X))$sigma^2,silent=TRUE))
{
n<-dim(X)[1] ; p<-dim(X)[2]
if(p==0) { s20<-mean(y^2) }
H0<-0 ; if(p>0) { H0<- (g/(g+1)) * X%*%solve(t(X)%*%X)%*%t(X) }
SS0<- t(y)%*%( diag(1,nrow=n)  - H0 ) %*%y
-.5*n*log(2*pi) +lgamma(.5*(nu0+n)) - lgamma(.5*nu0)  - .5*p*log(1+g) +
.5*nu0*log(.5*nu0*s20) -.5*(nu0+n)*log(.5*(nu0*s20+SS0))
}
####
mselect.gprior<-function(y,X,S=500*dim(X)[2],verb=FALSE)
{
Z<-NULL
z<-rep(1,dim(X)[2] )
for(s in 1:S)
{
for(j in sample(1:p))
{
z1<-z0<-z  ; z1[j]<-1 ; z0[j]<-0
r<-lmratio.gprior(z0,z1,y,X)
z[j]<-rbinom(1,1,1/(1+exp(-r)))
}
Z<-rbind(Z,z)
if(verb==TRUE) {cat(s,mean(z),"\n") }
}
Z
}
```
## Don't run it again if you've already run it
runmcmc<-!any(system("ls",intern=TRUE)=="data/diabetesBMA.RData")
if(!runmcmc){ load("data/diabetesBMA.RData") }
if(runmcmc){
BETA<-Z<-matrix(NA,S,p)
z<-rep(1,dim(X)[2] )
lpy.c<-lpy.X(y,X[,z==1,drop=FALSE])
for(s in 1:S)
{
for(j in sample(1:p))
{
zp<-z ; zp[j]<-1-zp[j]
lpy.p<-lpy.X(y,X[,zp==1,drop=FALSE])
r<- (lpy.p - lpy.c)*(-1)^(zp[j]==0)
z[j]<-rbinom(1,1,1/(1+exp(-r)))
if(z[j]==zp[j]) {lpy.c<-lpy.p}
}
beta<-z
if(sum(z)>0){beta[z==1]<-lm.gprior(y,X[,z==1,drop=FALSE],S=1)$beta }
Z[s,]<-z
BETA[s,]<-beta
if(s%%10==0)
{
bpm<-apply(BETA[1:s,],2,mean) ; plot(bpm)
cat(s,mean(z), mean( (y.te-X.te%*%bpm)^2),"\n")
Zcp<- apply(Z[1:s,,drop=FALSE],2,cumsum)/(1:s)
plot(c(1,s),range(Zcp),type="n") ; apply(Zcp,2,lines)
}
}
save(BETA,Z,file="diabetesBMA.RData")
}
par(mar=c(2.75,2.75,.5,.5),mgp=c(1.7,.7,0))
beta.bma<-apply(BETA,2,mean,na.rm=TRUE)
y.te.bma<-X.te%*%beta.bma
mean( (y.te-y.te.bma)^2)
layout( matrix(c(1,1,2),nrow=1,ncol=3) )
plot(apply(Z,2,mean,na.rm=TRUE),xlab="regressor index",ylab=expression(
paste( "Pr(",italic(z[j] == 1),"|",italic(y),",X)",sep="")),type="h",lwd=2)
plot(y.te,y.te.bma,xlab=expression(italic(y)[test]),
ylab=expression(hat(italic(y))[test])) ; abline(0,1)
#### Functions for variable selection
source("regression_gprior.R")
#### Functions for variable selection
source("regression_gprior.R")
source("backselect.R")
getwd()
#### Functions for variable selection
source("data/regression_gprior.R")
source("data/backselect.R")
par(mar=c(3,3,1,1),mgp=c(1.75,.75,0))
x1<-c(0,0,0,0,0,0,1,1,1,1,1,1)
x2<-c(23,22,22,25,27,20,31,23,27,28,22,24)
y<-c(-0.87,-10.74,-3.27,-1.97,7.50,-7.25,17.05,4.96,10.40,11.05,0.26,2.51)
par(mfrow=c(1,1))
plot(y~x2,pch=16,xlab="age",ylab="change in maximal oxygen uptake",
col=c("black","gray")[x1+1])
legend(27,0,legend=c("aerobic","running"),pch=c(16,16),col=c("gray","black"))
par(mfrow=c(2,2),mar=c(3,3,1,1),mgp=c(1.75,.75,0),oma=c(0,0,.25,0))
plot(y~x2,pch=16,col=c("black","gray")[x1+1],ylab="change in maximal oxygen uptake",xlab="",xaxt="n")
abline(h=mean(y[x1==0]),col="black")
abline(h=mean(y[x1==1]),col="gray")
mtext(side=3,expression(paste(beta[3]==0,"  ",beta[4]==0)) )
plot(y~x2,pch=16,col=c("black","gray")[x1+1],xlab="",ylab="",xaxt="n",yaxt="n")
abline(lm(y~x2),col="black")
abline(lm((y+.5)~x2),col="gray")
mtext(side=3,expression(paste(beta[2]==0,"  ",beta[4]==0)) )
plot(y~x2,pch=16,col=c("black","gray")[x1+1],
xlab="age",ylab="change in maximal oxygen uptake" )
fit<-lm( y~x1+x2)
abline(a=fit$coef[1],b=fit$coef[3],col="black")
abline(a=fit$coef[1]+fit$coef[2],b=fit$coef[3],col="gray")
mtext(side=3,expression(beta[4]==0))
plot(y~x2,pch=16,col=c("black","gray")[x1+1],
xlab="age",ylab="",yaxt="n")
abline(lm(y[x1==0]~x2[x1==0]),col="black")
abline(lm(y[x1==1]~x2[x1==1]),col="gray")
X<-cbind(rep(1,n),x1,x2,x1*x2)
#### OLS estimation
n<-length(y)
X<-cbind(rep(1,n),x1,x2,x1*x2)
X
dimX
dim(X)
p<-dim(X)[2]
solve(t(X)%*%X)%*%t(X)%*%y
beta.ols<- solve(t(X)%*%X)%*%t(X)%*%y
?chol
X
t(X)%*%X
t(X)%*%y
y
beta.ols
par(mfrow=c(2,2),mar=c(3,3,1,1),mgp=c(1.75,.75,0),oma=c(0,0,.25,0))
plot(y~x2,pch=16,col=c("black","gray")[x1+1],ylab="change in maximal oxygen uptake",xlab="",xaxt="n")
abline(h=mean(y[x1==0]),col="black")
abline(h=mean(y[x1==1]),col="gray")
mtext(side=3,expression(paste(beta[3]==0,"  ",beta[4]==0)) )
plot(y~x2,pch=16,col=c("black","gray")[x1+1],xlab="",ylab="",xaxt="n",yaxt="n")
abline(lm(y~x2),col="black")
abline(lm((y+.5)~x2),col="gray")
mtext(side=3,expression(paste(beta[2]==0,"  ",beta[4]==0)) )
plot(y~x2,pch=16,col=c("black","gray")[x1+1],
xlab="age",ylab="change in maximal oxygen uptake" )
fit<-lm( y~x1+x2)
abline(a=fit$coef[1],b=fit$coef[3],col="black")
abline(a=fit$coef[1]+fit$coef[2],b=fit$coef[3],col="gray")
mtext(side=3,expression(beta[4]==0))
plot(y~x2,pch=16,col=c("black","gray")[x1+1],
xlab="age",ylab="",yaxt="n")
abline(lm(y[x1==0]~x2[x1==0]),col="black")
abline(lm(y[x1==1]~x2[x1==1]),col="gray")
y
x1
x2
lm(y[x1==0]~x2[x1==0])
summary(lm(y[x1==0]~x2[x1==0]))
summary(lm(y[x1==1]~x2[x1==1]))
n
p
solve(t(X)%*%X)
beta.ols
sigma2.0<-sum(fit.ls$res^2)/(n-p)
#### Bayesian estimation via MCMC
n<-length(y)
X<-cbind(rep(1,n),x1,x2,x1*x2)
p<-dim(X)[2]
fit.ls<-lm(y~-1+ X)
beta.0<-rep(0,p) ; Sigma.0<-diag(c(150,30,6,5)^2,p)
beta.0<-rep(0,p) ; Sigma.0<-diag(c(150,30,6,5)^2,p)
nu.0<-1 ; sigma2.0<- 15^2
beta.0<-fit.ls$coef
nu.0<-1  ; sigma2.0<-sum(fit.ls$res^2)/(n-p)
Sigma.0<- solve(t(X)%*%X)*sigma2.0*n
sigma2.0
Sigma.0
X
beta.ols
X %*% beta.ols
y
y - X %*% beta.ols
(y - X %*% beta.ols)^2
sum((y - X %*% beta.ols)^2)
sum((y - X %*% beta.ols)^2)/(n-p)
solve(t(X)%*%X)
sum((y - X %*% beta.ols)^2)/(n-p)
X %*% beta.ols
y - X %*% beta.ols
solve(t(X)%*%X)
solve(t(X)%*%X) * sum((y - X %*% beta.ols)^2)/(n-p)
diag(solve(t(X)%*%X) * sum((y - X %*% beta.ols)^2)/(n-p))
sqrt(diag(solve(t(X)%*%X) * sum((y - X %*% beta.ols)^2)/(n-p)))
beta.ols
sum((y - X %*% beta.ols)^2)/(n-p)
